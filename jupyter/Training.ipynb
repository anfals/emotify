{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B92L6Se2Fglo"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWL6eBLIFJRn"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q emoji\n",
        "!pip install -q ray\n",
        "!pip install pickle5==0.0.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIg3uevsFoff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, RobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-CnKhG8Fp0O"
      },
      "source": [
        "# Dataset Installation\n",
        "We make use of the [TweetEval](https://huggingface.co/datasets/tweet_eval) dataset, particularly its \"emoji\" subdataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFmzbOGcFw_t"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
        "\n",
        "dataset\n",
        "dataset['test'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryhQ5oQ8J_mR"
      },
      "outputs": [],
      "source": [
        "emoji_labels = dataset['train'].features['label'].names\n",
        "num_labels = len(emoji_labels)\n",
        "id2label = dict(zip(range(num_labels), emoji_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1sZBzyYGVdN"
      },
      "source": [
        "## Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCl5CS_pGZFk"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True, normalization=True)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy2wMYD0G4jd"
      },
      "source": [
        "# Training\n",
        "We leverage pre-trained base model as described in https://aclanthology.org/2020.emnlp-demos.2.pdf and https://huggingface.co/docs/transformers/model_doc/bertweet, which we fine-tune using Emoji data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT9w_1VwIThY"
      },
      "source": [
        "## Hyperparameter Tuning Using Ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxow_49CISJM"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\", \n",
        "                  evaluation_strategy=\"steps\", eval_steps=500, num_train_epochs=3\n",
        "                  )\n",
        "\n",
        "tune_config = {\n",
        "        \"per_device_train_batch_size\": 8,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"classifier_dropout\": tune.uniform(0, 0.5),\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"learning_rate\": tune.loguniform(1e-6, 1e-4)\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"f1\": f1_score(labels, predictions, average='macro')}\n",
        "\n",
        "def model_init(trial):\n",
        "  if trial == None:\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=num_labels, problem_type=\"single_label_classification\")\n",
        "  else:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=num_labels, problem_type=\"single_label_classification\", classifier_dropout=trial['classifier_dropout'])\n",
        "    return model\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'].shard(num_shards=10, index=3),\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "br = trainer.hyperparameter_search(\n",
        "    hp_space=lambda _: tune_config,\n",
        "    direction=\"maximize\", \n",
        "    backend=\"ray\", \n",
        "    n_trials=1, # number of trials\n",
        "    scheduler=ASHAScheduler(metric=\"objective\", mode=\"max\")\n",
        ")\n",
        "\n",
        "br\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifOk_riyI5X7"
      },
      "source": [
        "## Initiate Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yymik1xeI9Dz"
      },
      "outputs": [],
      "source": [
        "################## YOUR CODE HERE ##################\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "hyp = br.hyperparameters\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", \n",
        "                                                           num_labels=num_labels, \n",
        "                                                           problem_type=\"single_label_classification\",\n",
        "                                                           classifier_dropout=hyp['classifier_dropout'])\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Setup training\n",
        "training_args = TrainingArguments(\"test_trainer\", \n",
        "                  num_train_epochs=1,\n",
        "                  per_device_train_batch_size=8,\n",
        "                  per_device_eval_batch_size=32,\n",
        "                  evaluation_strategy=\"epoch\",\n",
        "                  save_strategy=\"epoch\",                \n",
        "                  learning_rate=hyp['learning_rate'],\n",
        "                  weight_decay=hyp['weight_decay'],               \n",
        "                  metric_for_best_model = 'f1',\n",
        "                  load_best_model_at_end=True\n",
        "                  )\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    ret = metric.compute(predictions=predictions, references=labels)\n",
        "    ret['f1'] = f1_score(labels, predictions, average='macro')\n",
        "    return ret\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "aEOB8JplLOWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(tokenized_datasets['test'])"
      ],
      "metadata": {
        "id": "4gutYyq4LPmK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}